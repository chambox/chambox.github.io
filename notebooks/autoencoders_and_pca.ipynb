{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Autoencoder vs Principal component analysis (PCA)\n",
    "\n",
    "An autoencoder is a special type of neural network which computes nonlinear principal component  decomposition. \n",
    "\n",
    "The linear PCA is a very popular method for dimension reduction and data visualisation.  What PCA does is reducing a large set  of correlated variables to a typically smaller number of linear combinations that captures most the variance in the original set.  \n",
    "\n",
    "More concretely, given a collection of $n$ vectors $x_i\\in \\mathcal{R}^p$, we will assume these vectors have mean zero.  PCA produces a derived set of uncorrelated features $z_i\\in \\mathcal{R}^q$ (q\\leq p). The set $z_i$ are related to $x_i$ as follows \n",
    "$$\n",
    "z_i = \\mathbf{V}^\\prime x_i.\n",
    "$$\n",
    "\n",
    "The columns of $\\mathbf{V}$ are orthonormal and they are constructed such that the first component of $z_i$ has maximal variance, the second has the next largest variance and is uncorrelated with the first and so on. \n",
    "\n",
    "If you like to check you will also realise that the columns of $\\mathbf V$ are the leading $q$ eigenvectors fo the sample covariance matrox $S=\\frac{1}{n}\\mathbf{X}^\\prime \\mathbf{X}$.\n",
    "\n",
    "Principle components can also be formulated as a minimisation problem.  The solution is then the best-approximating linear subspace. This version is what then leads to the nonlinear generalisation we will discuss in-depth in this article. \n",
    "\n",
    "Consider the optimisation problem "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\text{minimize}_{\\mathbf{A\\in\\mathcal{R}^{p\\times q},\\{\\gamma_i\\}_1^n}\\in \\mathcal{R}^{q\\times n}} \\sum_{i=1}^{n}\\|x_i-\\mathbf A\\gamma_i\\|^{2}_{2}\n",
    "$$\n",
    "\n",
    "for $q \\leq q$. The subspace is defined by the column space of $\\mathbf A$, \n",
    "and for each point $x_i$ we wish to locate its best approximation in the subspace. This we do interms of Euclidean distance. Without loss of generality, we can assume $\\mathbf A$ ijas orthonormal columns, in which case $\\hat \\gamma_i = \\mathbf A^\\prime x_i$ for each $i$ ($n$ separate linear regressions). Plugging in the equation above gives \n",
    "\n",
    "$$\n",
    "\\text{minimize}_{\\mathbf{A\\in\\mathcal{R}^{p\\times q},\\{\\gamma_i\\}_1^n}\\in \\mathcal{R}^{q\\times n}} \\sum_{i=1}^{n}\\|x_i-\\mathbf A \\mathbf A^\\prime x_i\\|^{2}_{2}.\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The solution is given by $\\mathbf{\\hat A}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "2fb4dcad2038b855acc5d2d9285caf9743f26bff08ac6b66b0399088ab57b0e2"
  },
  "kernelspec": {
   "display_name": "Python 3.7.11 64-bit ('venv': venv)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
