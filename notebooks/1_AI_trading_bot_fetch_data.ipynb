{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "This blog post is the first in the series of `building an AI (Artificial intelligence) crypto trading bot`, the chambot. \n",
    "\n",
    "The assumption we make here is that you are already familiar with how the stock, crypto, or foreign exchange work. If we pick crypto as our financial instrument without loss of generality, you will notice that the price of crypto is always moving up and down. This movement is risky for an investor who might incur huge losses if the crypto price drops significantly after buying at a higher price. On the other hand, traders that buy and sell crypto try to use these fluctuations in crypto prices to their advantage.\n",
    "\n",
    "They buy when the price is low, and sell when the price is high. The real question is when do you buy (hold a position) and when do you sell (leave a position). Many traders spend hours staring at the computer screen waiting for the right moment to act.  In a couple of blog posts, I will show how to build a robot that automatically buys at the low and sell at a high price. This bot is called the chambot and uses AI for smart buying and selling. We will need data to do this,  AI models need that has predictor variables and a target. Our workflow will reflect this. \n",
    "\n",
    "Here is the work flow:\n",
    "\n",
    "![](../../images/flow_diagram.png)\n",
    "\n",
    "These are the steps we will take:\n",
    "1. Fetch market data from [Binance](https://www.binance.com/en)\n",
    "2. Create a target variable\n",
    "3. Create predictor variables\n",
    "4. Merge the predictor variables and the target variable to form a dataset that will be used to train an AI model\n",
    "5. Train AI model\n",
    "6. Put the model in production on AWS to start buying and selling crypto and making huge profits. \n",
    "\n",
    "This short blog post is based on point 1., i.e., fetching data from binance, I will show you how to fetch data from [Binance](https://www.binance.com/en) and store it as a `.csv`. \n",
    "\n",
    "Make sure your `requirements.txt` contains the following modules.\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "import requests\n",
    "import json \n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "import datetime as dt \n",
    "from binance.client import Client\n",
    "import time\n",
    "from datetime import timedelta, datetime\n",
    "import math\n",
    "from dateutil import parser\n",
    "import os.path\n",
    "import config ### this config file should contain your API keys API_KEY_BINANCE and API_SECRET_BINANCE\n",
    "              ### ofcourse I will not put mine here :) "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "We will define two important functions that will fetch data and download the data. We will set the `batch_size` and the `binsizes`, as the name implies these are the batch size and the bin sizes.  We will also create a binance client for subsequent use."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "binsizes = {\"1m\": 1, \"15m\": 15, \"1h\": 60, \"1d\": 1440}\n",
    "batch_size = 750\n",
    "result = pd.DataFrame(binsizes,index=[0])\n",
    "\n",
    "binance_client = Client(api_key=config.API_KEY_BINANCE, \n",
    "                        api_secret=config.API_SECRET_BINANCE)\n",
    "                        \n",
    "print(result.to_markdown())"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "|    |   1m |   15m |   1h |   1d |\n",
      "|---:|-----:|------:|-----:|-----:|\n",
      "|  0 |    1 |    15 |   60 | 1440 |\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "## FUNCTIONS\n",
    "def minutes_of_new_data(symbol, kline_size, data, source):\n",
    "    if len(data) > 0:  old = parser.parse(data[\"timestamp\"].iloc[-1])\n",
    "    elif source == \"binance\": old = datetime.strptime('1 Jan 2017', '%d %b %Y')\n",
    "    # elif source == \"bitmex\": old = bitmex_client.Trade.Trade_getBucketed(symbol=symbol, binSize=kline_size, count=1, reverse=False).result()[0][0]['timestamp']\n",
    "    if source == \"binance\": new = pd.to_datetime(binance_client.get_klines(symbol=symbol, interval=kline_size)[-1][0], unit='ms')\n",
    "    # if source == \"bitmex\": new = bitmex_client.Trade.Trade_getBucketed(symbol=symbol, binSize=kline_size, count=1, reverse=True).result()[0][0]['timestamp']\n",
    "    return old, new\n",
    "\n",
    "def get_all_binance(symbol, kline_size, save = False):\n",
    "\n",
    "    filename = '%s-%s-data.csv' % (symbol, kline_size)\n",
    "    if os.path.isfile(filename): data_df = pd.read_csv(filename)\n",
    "\n",
    "    else: data_df = pd.DataFrame()\n",
    "    oldest_point, newest_point = minutes_of_new_data(symbol, \n",
    "                                                     kline_size, \n",
    "                                                     data_df, \n",
    "                                                     source = \"binance\")\n",
    "\n",
    "    delta_min = (newest_point - oldest_point).total_seconds()/60\n",
    "    available_data = math.ceil(delta_min/binsizes[kline_size])\n",
    "\n",
    "    if oldest_point == datetime.strptime('1 Jan 2017', '%d %b %Y'): \n",
    "        print(f'Downloading all available {kline_size} data for {symbol}. Be patient..!')\n",
    "    else: \n",
    "        print(f'Downloading {delta_min} minutes of new data available for {symbol}, i.e. {available_data} instances of {kline_size} data.')\n",
    "    klines =( binance_client.get_historical_klines(symbol, kline_size, oldest_point.strftime(\"%d %b %Y %H:%M:%S\"), newest_point.strftime(\"%d %b %Y %H:%M:%S\")))\n",
    "    data = pd.DataFrame(klines, columns = ['timestamp', \n",
    "                                           'open', 'high', 'low', \n",
    "                                           'close', 'volume', 'close_time', \n",
    "                                           'quote_av', 'trades', \n",
    "                                           'tb_base_av', \n",
    "                                           'tb_quote_av', 'ignore' ])\n",
    "\n",
    "    data['timestamp'] = pd.to_datetime(data['timestamp'], unit='ms')\n",
    "    if len(data_df) > 0:\n",
    "        temp_df = pd.DataFrame(data)\n",
    "        data_df = data_df.append(temp_df)\n",
    "    else: \n",
    "        data_df = data\n",
    "    data_df.set_index('timestamp', inplace=True)\n",
    "    if save: \n",
    "        data_df.to_csv(f'{filename}')\n",
    "    print('All caught up..!')\n",
    "    return data_df\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "`USDT` is a stable crypto currency that mimics the US dollar in the crypto world. Often it is  tractable if you trade other volatile crypto currency like the Ethereum (`ETH`) with `USDT`. So we will first grab all `X-USDT` pairs from Binance. `X` stands for any crypto currency."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "tickers = requests.get('https://www.binance.com/api/v1/ticker/allPrices').text\n",
    "tickers = pd.DataFrame(json.loads(tickers))['symbol'].values\n",
    "tickers = tickers[[tk.find('USDT') not in [0,-1] for  tk in tickers]]\n",
    "tickers[0:20]"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array(['BTCUSDT', 'ETHUSDT', 'BNBUSDT', 'BCCUSDT', 'NEOUSDT', 'LTCUSDT',\n",
       "       'QTUMUSDT', 'ADAUSDT', 'XRPUSDT', 'EOSUSDT', 'TUSDUSDT',\n",
       "       'IOTAUSDT', 'XLMUSDT', 'ONTUSDT', 'TRXUSDT', 'ETCUSDT', 'ICXUSDT',\n",
       "       'VENUSDT', 'NULSUSDT', 'VETUSDT'], dtype=object)"
      ]
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can now loop through the `tickers` list and download the data for all `XUSDT` pairs. That will take quite sometime. To download the data for a single group we do as follows:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "tickers=['ETHUSDT']\n",
    "for symbol in tickers:\n",
    "    df = get_all_binance(symbol, '15m', save = True)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Downloading 15.0 minutes of new data available for ETHUSDT, i.e. 1 instances of 15m data.\n",
      "All caught up..!\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Hope fully everything has worked fine.  Let's visualise the data that was downloaded:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "print(df.head().to_markdown())"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "| timestamp           |   open |   high |    low |   close |   volume |    close_time |   quote_av |   trades |   tb_base_av |   tb_quote_av |   ignore |\n",
      "|:--------------------|-------:|-------:|-------:|--------:|---------:|--------------:|-----------:|---------:|-------------:|--------------:|---------:|\n",
      "| 2017-08-17 04:00:00 | 301.13 | 301.13 | 298    |  298    |  5.80167 | 1502943299999 |    1744.77 |       22 |      5.48392 |       1649.45 |  46528.3 |\n",
      "| 2017-08-17 04:15:00 | 298    | 300.8  | 298    |  299.39 | 31.4407  | 1502944199999 |    9396.92 |       26 |     12.1171  |       3625.17 |  46537.3 |\n",
      "| 2017-08-17 04:30:00 | 299.39 | 300.79 | 299.39 |  299.6  | 52.9358  | 1502945099999 |   15851.1  |       39 |     28.3816  |       8499.79 |  46678.9 |\n",
      "| 2017-08-17 04:45:00 | 299.6  | 302.57 | 299.6  |  301.61 | 35.4907  | 1502945999999 |   10692    |       42 |     34.5811  |      10419    |  47039.7 |\n",
      "| 2017-08-17 05:00:00 | 301.61 | 302.57 | 300.95 |  302.01 | 81.6924  | 1502946899999 |   24620.7  |       52 |     80.2634  |      24189.8  |  47181.9 |\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "In subsequent blog posts, we will start building the `chambot`. Do not miss to be part of this exciting journey. "
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}